{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --quiet torch\n",
    "!pip3 install --quiet -q transformers datasets\n",
    "!pip3 install --quiet --upgrade scikit-learn==1.0.2\n",
    "!pip3 install --quiet matplotlib\n",
    "!pip3 install --quiet accelerate -U\n",
    "!pip install --quiet datasets -q\n",
    "!pip install --quiet wordcloud -q\n",
    "!pip install --quiet sentence-transformers -q\n",
    "!pip install --quiet nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pandas as pd\n",
    "import  matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from wordcloud import WordCloud\n",
    "from datasets import load_dataset\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type != 'cuda':\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "del dataset\n",
    "\n",
    "train_text = [train_dataset[i]['text'] for i in range(len(train_dataset))]\n",
    "train_label = [train_dataset[i]['label'] for i in range(len(train_dataset))]\n",
    "\n",
    "test_text = [test_dataset[i]['text'] for i in range(len(test_dataset))]\n",
    "test_label = [test_dataset[i]['label'] for i in range(len(test_dataset))]\n",
    "\n",
    "\n",
    "del train_dataset\n",
    "del test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting emotics from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 650000/650000 [00:12<00:00, 51752.46it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 53299.96it/s]\n"
     ]
    }
   ],
   "source": [
    "emoticon_regex = '(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)'\n",
    "train_text_noemot = [re.sub(emoticon_regex, '', tweet) for tweet in tqdm(train_text)]\n",
    "test_text_noemot  = [re.sub(emoticon_regex, '', tweet) for tweet in tqdm(test_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load model and move to the GPU\n",
    "model_name = 'bert-base-multilingual-uncased'\n",
    "bert = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(train_label)))\n",
    "bert = bert.to(device)\n",
    "\n",
    "#Load tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomSampling(X , Y , p):\n",
    "\n",
    "    unique_labels = np.unique(Y)\n",
    "\n",
    "    # Initialize lists to store sampled data\n",
    "    sampled_X = []\n",
    "    sampled_Y = []\n",
    "\n",
    "    # Iterate over each unique label\n",
    "    for label in unique_labels:\n",
    "        # Find indices corresponding to the current label\n",
    "        indices = np.where(np.array(Y )== label)[0]\n",
    "        # Randomly shuffle the indices\n",
    "        np.random.shuffle(indices)\n",
    "        num_samples = int(len(indices) * p)\n",
    "        if (num_samples<1):\n",
    "            print(f\"Not Enough samples for class {label}\")\n",
    "            return -1\n",
    "        sampled_indices = indices[:num_samples]\n",
    "        \n",
    "        # Append sampled data to the lists\n",
    "        sampled_X+=np.array(X)[sampled_indices.astype(int)].tolist()\n",
    "        sampled_Y+=np.array(Y)[sampled_indices.astype(int)].tolist()\n",
    "    \n",
    "    combined_data = list(zip(sampled_X, sampled_Y))\n",
    "\n",
    "    # Shuffle the combined data\n",
    "    np.random.shuffle(combined_data)\n",
    "\n",
    "    # Unzip the shuffled data back into X_train and Y_train\n",
    "    X_train_shuffled, Y_train_shuffled = zip(*combined_data)\n",
    "\n",
    "    # Convert back to lists if needed\n",
    "    X_train_shuffled = list(X_train_shuffled)\n",
    "    Y_train_shuffled = list(Y_train_shuffled)\n",
    "\n",
    "\n",
    "    return X_train_shuffled, Y_train_shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_text_noemot, train_label, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train,y_train = randomSampling(X_train, y_train , 0.1)\n",
    "\n",
    "X_val , y_val = randomSampling(X_val, y_val, 0.1)\n",
    "\n",
    "train_data = [{'text': txt, 'label': lbl} for txt, lbl in zip(X_train, y_train)]\n",
    "validation_data = [{'text': txt, 'label': lbl} for txt, lbl in zip(X_val, y_val)]\n",
    "test_data = [{'text': txt, 'label': lbl} for txt, lbl in zip(test_text_noemot, test_label)]\n",
    "\n",
    "#Convert to huggingface dataset api \n",
    "train_data = Dataset.from_list(train_data)\n",
    "validation_data = Dataset.from_list(validation_data)\n",
    "test_data = Dataset.from_list(test_data)\n",
    "\n",
    "\n",
    "\n",
    "data = DatasetDict()\n",
    "data['train'] = train_data\n",
    "data['validation'] = validation_data\n",
    "data['test'] = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45499, 2), (19497, 2), (50000, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].shape , data['validation'].shape , data['test'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to Tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa1dac71452443bb4c86e5b4e0be8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f21b86a50a4bd7bd8e48913ec48247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19497 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b32a119b3b4046b179ad202651d3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"Bert_V1_Training\", \n",
    "    #per_device_train_batch_size=16,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    half_precision_backend=True,\n",
    "    fp16_opt_level=True,\n",
    "    do_eval=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=bert, \n",
    "    args=training_args, \n",
    "    train_dataset=tokenized_data['train'], \n",
    "    eval_dataset=tokenized_data['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8532' max='8532' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8532/8532 1:21:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.219800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.991500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.973500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.949000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.880900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.797800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.809400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.786800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.788800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.767300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.664200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.595600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.604600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.582000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.596400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8532, training_loss=0.8025369431790309, metrics={'train_runtime': 4902.5603, 'train_samples_per_second': 27.842, 'train_steps_per_second': 1.74, 'total_flos': 3.591483709190861e+16, 'train_loss': 0.8025369431790309, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9240946769714355,\n",
       " 'eval_runtime': 202.7658,\n",
       " 'eval_samples_per_second': 96.155,\n",
       " 'eval_steps_per_second': 12.024,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.75      0.76     10000\n",
      "           1       0.58      0.59      0.59     10000\n",
      "           2       0.57      0.58      0.57     10000\n",
      "           3       0.55      0.56      0.56     10000\n",
      "           4       0.73      0.71      0.72     10000\n",
      "\n",
      "    accuracy                           0.64     50000\n",
      "   macro avg       0.64      0.64      0.64     50000\n",
      "weighted avg       0.64      0.64      0.64     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(tokenized_data['test'])\n",
    "y_pred = torch.argmax(torch.tensor(preds.predictions), dim=1).numpy()\n",
    "print(classification_report(tokenized_data['test']['label'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
